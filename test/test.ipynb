{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "441da649",
   "metadata": {},
   "source": [
    "## MalConv2 Model Execution\n",
    "This section demonstrates how to load and run the MalConv2 model using the instructions and files from the `MalConv2-main` directory. Code is based on the README instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf8adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 경로 설정 유연성 확보 (Notebook 실행 위치에 따라 경로가 다를 수 있음)\n",
    "# MalConv2-main 폴더 찾기\n",
    "current_dir = os.getcwd()\n",
    "possible_paths = [\n",
    "    os.path.join(current_dir, 'MalConv2-main'),         # 작업 설정이 루트일 경우\n",
    "    os.path.join(current_dir, '../models/MalConv2-main'),      # 작업 설정이 현재 파일 위치(test/)일 경우\n",
    "    '/Users/wjm/Desktop/2026 프로젝트/Binary-Hunter/models/MalConv2-main' # 절대 경로 (fallback)\n",
    "]\n",
    "\n",
    "malconv_path = None\n",
    "for p in possible_paths:\n",
    "    if os.path.exists(p) and os.path.isdir(p):\n",
    "        malconv_path = os.path.abspath(p)\n",
    "        break\n",
    "\n",
    "if malconv_path:\n",
    "    if malconv_path not in sys.path:\n",
    "        sys.path.append(malconv_path)\n",
    "    print(f\"MalConv2-main path added: {malconv_path}\")\n",
    "else:\n",
    "    print(\"Error: Could not find MalConv2-main directory.\")\n",
    "\n",
    "# src 폴더 경로 추가 (preprocess.py 사용을 위해)\n",
    "src_path = os.path.abspath(os.path.join(current_dir, '../'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "try:\n",
    "    # MalConvGCT 및 전처리 함수 임포트\n",
    "    from MalConvGCT_nocat import MalConvGCT\n",
    "    from src import preprocess_pe_file\n",
    "    print(\"Successfully imported MalConvGCT and preprocess_pe_file\")\n",
    "\n",
    "    # 모델 초기화 (README의 파라미터 참고)\n",
    "    # channels=256, window_size=256, stride=64 설정\n",
    "    channels = 256\n",
    "    window_size = 256\n",
    "    stride = 64\n",
    "    embd_size = 8\n",
    "    \n",
    "    print(\"Initializing model...\")\n",
    "    model = MalConvGCT(out_size=2, channels=channels, window_size=window_size, stride=stride, embd_size=embd_size)\n",
    "    \n",
    "    # 체크포인트 로드\n",
    "    checkpoint_path = os.path.join(malconv_path, 'malconvGCT_nocat.checkpoint')\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "        # Mac M2(Apple Silicon) 호환성을 위해 map_location='cpu' 사용 권장\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "        \n",
    "        # 가중치 로드\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "            print(\"Model weights loaded successfully.\")\n",
    "        else:\n",
    "            print(f\"Warning: 'model_state_dict' not found. Keys: {checkpoint.keys()}\")\n",
    "    else:\n",
    "        print(f\"Warning: Checkpoint file not found at {checkpoint_path}. Running with random weights.\")\n",
    "\n",
    "    # 모델을 평가 모드로 설정\n",
    "    model.eval()\n",
    "\n",
    "    # 실제 파일 전처리 및 실행 테스트\n",
    "    target_file = 'ZoomInstaller.exe'\n",
    "    \n",
    "    # 파일 존재 여부 확인 후 진행\n",
    "    if os.path.exists(target_file):\n",
    "        print(f\"Processing file: {target_file}\")\n",
    "        \n",
    "        # preprocess_pe_file 함수를 사용하여 입력 데이터 준비\n",
    "        # mode='default' 사용\n",
    "        input_tensor = preprocess_pe_file(target_file, mode='default')\n",
    "        \n",
    "        print(f\"Running inference on input with shape {input_tensor.shape}...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            # MalConvGCT returns: (logits, penult, post_conv)\n",
    "            logits = output[0]\n",
    "            probabilities = F.softmax(logits, dim=1)\n",
    "            \n",
    "        print(\"\\nExecution Complete!\")\n",
    "        print(f\"Logits: {logits}\")\n",
    "        \n",
    "        # 결과 해석\n",
    "        print(\"\\n[결과 해석]\")\n",
    "        print(f\"1. Logits (Raw Score): {logits.tolist()}\")\n",
    "        print(f\"2. Probabilities (Softmax): {probabilities.tolist()}\")\n",
    "        print(f\"   - Class 0 (Benign/정상): {probabilities[0][0].item():.4f} ({probabilities[0][0].item()*100:.2f}%)\")\n",
    "        print(f\"   - Class 1 (Malware/악성): {probabilities[0][1].item():.4f} ({probabilities[0][1].item()*100:.2f}%)\")\n",
    "        \n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "        class_label = \"악성 (Malware)\" if predicted_class == 1 else \"정상 (Benign)\"\n",
    "        print(f\"3. 최종 예측: {class_label} (Class {predicted_class})\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: Target file '{target_file}' not found. Please make sure the file exists in the current directory.\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"Import Error: {e}\")\n",
    "    print(\"Make sure MalConv2-main is in the path and src package is accessible.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config 로드 테스트\n",
    "from src.utils import load_config\n",
    "try:\n",
    "    config = load_config()\n",
    "    print(\"Config loaded successfully:\")\n",
    "    print(config)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load config: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MalConvGCTDeepShap Class Test\n",
    "# IMPORTANT: This cell reloads libraries to define necessary classes before running test logic.\n",
    "# Do not skip.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import shap\n",
    "import importlib\n",
    "import src.compute_DeepShap\n",
    "importlib.reload(src.compute_DeepShap)\n",
    "from src.compute_DeepShap import MalConvGCTDeepShap, DeepShapExplainer\n",
    "from src import preprocess_pe_file\n",
    "\n",
    "print(\"Library reloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b554407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MalConvGCTDeepShap Adversarial Attack & Comparison Test\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import shap\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "print(\"\\n--- Testing Adversarial Attack & DeepSHAP Comparison ---\")\n",
    "\n",
    "# 1. Path Setup\n",
    "current_dir = os.getcwd()\n",
    "src_path = os.path.abspath(os.path.join(current_dir, '../'))\n",
    "if src_path not in sys.path: sys.path.append(src_path)\n",
    "\n",
    "# MalConv2-main Path\n",
    "possible_paths = [\n",
    "    os.path.join(current_dir, '../models/MalConv2-main'),\n",
    "    os.path.join(current_dir, 'MalConv2-main'),\n",
    "    '/Users/wjm/Desktop/2026 프로젝트/Binary-Hunter/models/MalConv2-main'\n",
    "]\n",
    "\n",
    "malconv_path = None\n",
    "for p in possible_paths:\n",
    "    if os.path.exists(p) and os.path.isdir(p):\n",
    "        malconv_path = os.path.abspath(p)\n",
    "        break\n",
    "\n",
    "if malconv_path:\n",
    "    if malconv_path not in sys.path: sys.path.append(malconv_path)\n",
    "else:\n",
    "    print(\"Warning: MalConv2-main path not found.\")\n",
    "\n",
    "# Reload Modules\n",
    "try:\n",
    "    import src.compute_DeepShap\n",
    "    importlib.reload(src.compute_DeepShap)\n",
    "    import src.adversarial_malware\n",
    "    importlib.reload(src.adversarial_malware)\n",
    "except Exception as e:\n",
    "    print(f\"Module reload warning: {e}\")\n",
    "\n",
    "from src.compute_DeepShap import MalConvGCTDeepShap\n",
    "from src.adversarial_malware import generate_adversarial_example\n",
    "from src import preprocess_pe_file\n",
    "\n",
    "# Helper: Initialize New Model (Factory Function)\n",
    "def create_new_model():\n",
    "    \"\"\"\n",
    "    Creates a fresh instance of MalConvGCTDeepShap to avoid any state leakage.\n",
    "    \"\"\"\n",
    "    channels = 256\n",
    "    window_size = 256\n",
    "    stride = 64\n",
    "    embd_size = 8\n",
    "    \n",
    "    model = MalConvGCTDeepShap(out_size=2, channels=channels, window_size=window_size, stride=stride, embd_size=embd_size)\n",
    "    \n",
    "    if malconv_path:\n",
    "        checkpoint_path = os.path.join(malconv_path, 'malconvGCT_nocat.checkpoint')\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            try:\n",
    "                ckpt = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "                if 'model_state_dict' in ckpt:\n",
    "                    model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "                else:\n",
    "                    model.load_state_dict(ckpt, strict=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load checkpoint: {e}\")\n",
    "                \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# 4. Data Setup\n",
    "data_dir_candidates = [\n",
    "    os.path.join(current_dir, '../data'),\n",
    "    os.path.join(current_dir, 'data'),\n",
    "    '/Users/wjm/Desktop/2026 프로젝트/Binary-Hunter/data'\n",
    "]\n",
    "data_dir = None\n",
    "for d in data_dir_candidates:\n",
    "    if os.path.exists(d) and os.path.isdir(d):\n",
    "        data_dir = d\n",
    "        break\n",
    "\n",
    "if not data_dir:\n",
    "    print(\"Data directory not found!\")\n",
    "    files_to_process = []\n",
    "else:\n",
    "    files_to_process = [f for f in os.listdir(data_dir) if not f.startswith('.')]\n",
    "    files_to_process = files_to_process[:7] # Limit to 7 files\n",
    "    print(f\"Target Files (Max 7): {files_to_process}\")\n",
    "\n",
    "# Helper: Discrete Plot\n",
    "def plot_discrete_shap(ax, data, title, threshold=1e-5):\n",
    "    if data.ndim > 1: data = data.flatten()\n",
    "    \n",
    "    # Filter by threshold\n",
    "    active_mask = np.abs(data) > threshold\n",
    "    indices = np.where(active_mask)[0]\n",
    "    values = data[indices]\n",
    "    \n",
    "    if len(indices) == 0:\n",
    "        ax.text(0.5, 0.5, \"No significant contributions\", ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(title)\n",
    "        return\n",
    "\n",
    "    colors = ['red' if v > 0 else 'blue' for v in values]\n",
    "    \n",
    "    ax.vlines(indices, 0, values, colors=colors, linewidth=1.0, alpha=0.7)\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Impact')\n",
    "    ax.grid(True, alpha=0.2, axis='y')\n",
    "\n",
    "# Helper: Verification\n",
    "def verify_shap_additivity(model, shap_vals_sum, input_len, logic_logit, description):\n",
    "    \"\"\"\n",
    "    Verifies that Sum(SHAP) + BaseValue == ModelOutput.\n",
    "    BaseValue must be calculated using ZeroEmbeddings passed through the FIXED graph (saved indices).\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # 1. Prepare Baseline (Zero Embeddings)\n",
    "    # DeepSHAP uses Zero Embeddings as baseline\n",
    "    embd_size = model.embd_size\n",
    "    baseline_combined = torch.zeros((1, input_len, 2*embd_size)).to(device)\n",
    "    \n",
    "    # 2. Compute Base Value with FORCED Indices\n",
    "    # We must enable _is_explaining on both main and context models to use saved_indices\n",
    "    model._is_explaining = True\n",
    "    if hasattr(model, \"context_net\"):\n",
    "        model.context_net._is_explaining = True\n",
    "        \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            base_logits = model(baseline_combined)\n",
    "            # The model output is raw Logits.\n",
    "            # We are verifying Class 1 (Malware) Logit.\n",
    "            base_value = base_logits[0, 1].item()\n",
    "    finally:\n",
    "        model._is_explaining = False\n",
    "        if hasattr(model, \"context_net\"):\n",
    "            model.context_net._is_explaining = False\n",
    "            \n",
    "    # 3. Compare\n",
    "    # SHAP Sum represents the change from Expected Value (Base Value) to Output\n",
    "    # So: Output ≈ BaseValue + Sum(SHAP)\n",
    "    reconstructed = shap_vals_sum + base_value\n",
    "    diff = abs(reconstructed - logic_logit)\n",
    "    \n",
    "    print(f\"  [{description}] Verifiction:\")\n",
    "    print(f\"    SHAP Sum : {shap_vals_sum:.4f}\")\n",
    "    print(f\"    Base Val : {base_value:.4f} (Model output on Zero Embeddings with Fixed Indices)\")\n",
    "    print(f\"    Sum+Base : {reconstructed:.4f}\")\n",
    "    print(f\"    Model Out: {logic_logit:.4f}\")\n",
    "    print(f\"    Diff     : {diff:.6f}\")\n",
    "    \n",
    "    if diff < 1.0:\n",
    "        print(\"    >> PASS: Additivity Verified.\")\n",
    "    else:\n",
    "        print(\"    >> FAIL: Additivity Mismatch.\")\n",
    "        \n",
    "\n",
    "# 5. Process Loop\n",
    "for idx, filename in enumerate(files_to_process):\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    print(f\"\\n[{idx+1}/{len(files_to_process)}] Processing: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        # Read Original Bytes\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_bytes = f.read(4000000) # Max 4MB \n",
    "        orig_bytes = np.frombuffer(raw_bytes, dtype=np.uint8)\n",
    "        \n",
    "        # --- PHASE 1: Original Sample Analysis ---\n",
    "        # Create FRESH model instance for Original\n",
    "        model_orig = create_new_model()\n",
    "        \n",
    "        orig_tensor = torch.tensor(orig_bytes.astype(np.int32) + 1, dtype=torch.long).unsqueeze(0)\n",
    "        \n",
    "        # Inference & SHAP (Original)\n",
    "        # 1. Clear saved indices to force fresh computation\n",
    "        if hasattr(model_orig, \"saved_indices\"): model_orig.saved_indices = None\n",
    "        if hasattr(model_orig.context_net, \"saved_indices\"): model_orig.context_net.saved_indices = None\n",
    "        \n",
    "        out_orig = model_orig(orig_tensor)\n",
    "        logits_orig = out_orig[0]\n",
    "        prob_orig = torch.nn.functional.softmax(logits_orig, dim=1)[0, 1].item()\n",
    "        logit_orig_val = logits_orig[0, 1].item()\n",
    "        shap_ctx_orig, shap_feat_orig = out_orig[3], out_orig[4]\n",
    "        \n",
    "        # Verify Original\n",
    "        shap_sum_orig = np.sum(shap_ctx_orig) + np.sum(shap_feat_orig)\n",
    "        verify_shap_additivity(model_orig, shap_sum_orig, len(orig_bytes), logit_orig_val, \"Original\")\n",
    "        \n",
    "        # --- PHASE 2: Attack Generation ---\n",
    "        # Generate Adversarial Example using the Original Model\n",
    "        # print(\"  Generating Adversarial Example...\")\n",
    "        adv_bytes = generate_adversarial_example(model_orig, orig_bytes, target_class=0)\n",
    "        \n",
    "        # Clean up model_orig to free memory/state\n",
    "        del model_orig\n",
    "        \n",
    "        # --- PHASE 3: Adversarial Sample Analysis ---\n",
    "        # Create FRESH model instance for Adversarial\n",
    "        model_adv = create_new_model()\n",
    "        \n",
    "        adv_tensor = torch.tensor(adv_bytes.astype(np.int32) + 1, dtype=torch.long).unsqueeze(0)\n",
    "        \n",
    "        # Inference & SHAP (Adversarial)\n",
    "        # 1. Clear saved indices to force fresh computation\n",
    "        if hasattr(model_adv, \"saved_indices\"): model_adv.saved_indices = None\n",
    "        if hasattr(model_adv.context_net, \"saved_indices\"): model_adv.context_net.saved_indices = None\n",
    "\n",
    "        out_adv = model_adv(adv_tensor)\n",
    "        logits_adv = out_adv[0]\n",
    "        prob_adv = torch.nn.functional.softmax(logits_adv, dim=1)[0, 1].item()\n",
    "        logit_adv_val = logits_adv[0, 1].item()\n",
    "        shap_ctx_adv, shap_feat_adv = out_adv[3], out_adv[4]\n",
    "        \n",
    "        print(f\"  Result: Original Prob={prob_orig*100:.2f}% -> Adversarial Prob={prob_adv*100:.2f}%\")\n",
    "        \n",
    "        # Verify Adversarial\n",
    "        shap_sum_adv = np.sum(shap_ctx_adv) + np.sum(shap_feat_adv)\n",
    "        verify_shap_additivity(model_adv, shap_sum_adv, len(adv_bytes), logit_adv_val, \"Adversarial\")\n",
    "\n",
    "\n",
    "        # 5. Visualization (Discrete Style)\n",
    "        fig, axes = plt.subplots(4, 1, figsize=(15, 12), sharex=True)\n",
    "        \n",
    "        # Filter significant values for cleaner plot\n",
    "        vis_threshold = 1e-5\n",
    "        \n",
    "        plot_discrete_shap(axes[0], shap_ctx_orig, f'Original - Context (Prob: {prob_orig:.4f})', vis_threshold)\n",
    "        plot_discrete_shap(axes[1], shap_feat_orig, 'Original - Feature', vis_threshold)\n",
    "        plot_discrete_shap(axes[2], shap_ctx_adv, f'Adversarial - Context (Prob: {prob_adv:.4f})', vis_threshold)\n",
    "        plot_discrete_shap(axes[3], shap_feat_adv, 'Adversarial - Feature', vis_threshold)\n",
    "        \n",
    "        # Force X-Axis to show full file (Adversarial Length)\n",
    "        total_len = len(adv_bytes)\n",
    "        axes[3].set_xlim(0, total_len)\n",
    "        \n",
    "        custom_lines = [Line2D([0], [0], color='red', lw=2),\n",
    "                        Line2D([0], [0], color='blue', lw=2)]\n",
    "        fig.legend(custom_lines, ['Increases Malware Score', 'Decreases Malware Score'], loc='upper right')\n",
    "        \n",
    "        axes[3].set_xlabel('Byte Index from Start of File')\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.93)\n",
    "        plt.show()\n",
    "        \n",
    "        # Cleanup\n",
    "        del model_adv\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\nComparison Complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
